{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd #Analysis \n",
    "import matplotlib.pyplot as plt #Visulization\n",
    "import seaborn as sns #Visulization\n",
    "import numpy as np #Analysis \n",
    "from scipy.stats import norm #Analysis \n",
    "from sklearn.preprocessing import StandardScaler #Analysis \n",
    "from scipy import stats #Analysis \n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "import gc\n",
    "\n",
    "import os\n",
    "import string\n",
    "color = sns.color_palette()\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from plotly import tools\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "from sklearn import model_selection, preprocessing, metrics, ensemble, naive_bayes, linear_model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import lightgbm as lgb\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_columns = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2db18f8c9faff8d19ecb0d3aff3969b37d4bc7ed"
   },
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../input/kuc-hackathon-winter-2018/drugsComTrain_raw.csv\", parse_dates=[\"date\"])\n",
    "df_test = pd.read_csv(\"../input/kuc-hackathon-winter-2018/drugsComTest_raw.csv\", parse_dates=[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3ab95e3f3ad6f51426d9732fabb29398c50223b8"
   },
   "outputs": [],
   "source": [
    "print(\"Train shape :\" ,df_train.shape)\n",
    "print(\"Test shape :\", df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7fd9b5423836819b3841d444b91492854ab8865a"
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8bf7685ab8278b0709c391aed2560386536552a4"
   },
   "outputs": [],
   "source": [
    "print(\"unique values count of train : \" ,len(set(df_train['uniqueID'].values)))\n",
    "print(\"length of train : \" ,df_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "1ce8f6dad73bddd2bf2c2a3a9cab10f2595679c2"
   },
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_train,df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bb2b7aac9798cb96aa4cd032aee6fca2ddb74b4a"
   },
   "outputs": [],
   "source": [
    "condition_dn = df_all.groupby(['condition'])['drugName'].nunique().sort_values(ascending=False)\n",
    "condition_dn[0:20].plot(kind=\"bar\", figsize = (14,6), fontsize = 10,color=\"green\")\n",
    "plt.xlabel(\"\", fontsize = 20)\n",
    "plt.ylabel(\"\", fontsize = 20)\n",
    "plt.title(\"Top20 : The number of drugs per condition.\", fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d0a89c7ebae84c817b3f2e6aab294088c18b261f"
   },
   "outputs": [],
   "source": [
    "df_all[df_all['condition']=='3</span> users found this comment helpful.'].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f46bf457b8c4b0a7b4ac2d476f42e38323ca483a"
   },
   "outputs": [],
   "source": [
    "condition_dn = df_all.groupby(['condition'])['drugName'].nunique().sort_values(ascending=False)\n",
    "\n",
    "condition_dn[condition_dn.shape[0]-20:condition_dn.shape[0]].plot(kind=\"bar\", figsize = (14,6), fontsize = 10,color=\"green\")\n",
    "plt.xlabel(\"\", fontsize = 20)\n",
    "plt.ylabel(\"\", fontsize = 20)\n",
    "plt.title(\"Bottom20 : The number of drugs per condition.\", fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1c283d11b69ada1b9defd9f5c9fab9d69022b631"
   },
   "outputs": [],
   "source": [
    "df_train['review'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c7d59b1d8ab8cd481ef1d99b91862fbcd34032d4"
   },
   "outputs": [],
   "source": [
    "df_train['review'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "ba8744fb8f95488d63b517eb64d61ee43afbee0d"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "def plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), \n",
    "                   title = None, title_size=40, image_color=False):\n",
    "    stopwords = set(STOPWORDS)\n",
    "    more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}\n",
    "    stopwords = stopwords.union(more_stopwords)\n",
    "\n",
    "    wordcloud = WordCloud(background_color='white',\n",
    "                    stopwords = stopwords,\n",
    "                    max_words = max_words,\n",
    "                    max_font_size = max_font_size, \n",
    "                    random_state = 42,\n",
    "                    width=800, \n",
    "                    height=400,\n",
    "                    mask = mask)\n",
    "    wordcloud.generate(str(text))\n",
    "    \n",
    "    plt.figure(figsize=figure_size)\n",
    "    if image_color:\n",
    "        image_colors = ImageColorGenerator(mask);\n",
    "        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n",
    "        plt.title(title, fontdict={'size': title_size,  \n",
    "                                  'verticalalignment': 'bottom'})\n",
    "    else:\n",
    "        plt.imshow(wordcloud);\n",
    "        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n",
    "                                  'verticalalignment': 'bottom'})\n",
    "    plt.axis('off');\n",
    "    plt.tight_layout()  \n",
    "    \n",
    "plot_wordcloud(df_all[\"review\"], title=\"Word Cloud of review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "72569a2d173760ec20f1893ee67c74e42773f186"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "df_all_6_10 = df_all[df_all[\"rating\"]>5]\n",
    "df_all_1_5 = df_all[df_all[\"rating\"]<6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "2d28e3c380acba0078318c33063d51ee86943555"
   },
   "outputs": [],
   "source": [
    "## custom function for ngram generation ##\n",
    "def generate_ngrams(text, n_gram=1):\n",
    "    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n",
    "    ngrams = zip(*[token[i:] for i in range(n_gram)])\n",
    "    return [\" \".join(ngram) for ngram in ngrams]\n",
    "\n",
    "## custom function for horizontal bar chart ##\n",
    "def horizontal_bar_chart(df, color):\n",
    "    trace = go.Bar(\n",
    "        y=df[\"word\"].values[::-1],\n",
    "        x=df[\"wordcount\"].values[::-1],\n",
    "        showlegend=False,\n",
    "        orientation = 'h',\n",
    "        marker=dict(\n",
    "            color=color,\n",
    "        ),\n",
    "    )\n",
    "    return trace\n",
    "\n",
    "## Get the bar chart from rating  8 to 10 review ##\n",
    "freq_dict = defaultdict(int)\n",
    "for sent in df_all_1_5[\"review\"]:\n",
    "    for word in generate_ngrams(sent):\n",
    "        freq_dict[word] += 1\n",
    "fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n",
    "fd_sorted.columns = [\"word\", \"wordcount\"]\n",
    "trace0 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n",
    "\n",
    "## Get the bar chart from rating  4 to 7 review ##\n",
    "freq_dict = defaultdict(int)\n",
    "for sent in df_all_6_10[\"review\"]:\n",
    "    for word in generate_ngrams(sent):\n",
    "        freq_dict[word] += 1\n",
    "fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n",
    "fd_sorted.columns = [\"word\", \"wordcount\"]\n",
    "trace1 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n",
    "\n",
    "# Creating two subplots\n",
    "fig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,\n",
    "                          subplot_titles=[\"Frequent words of rating 1 to 5\", \n",
    "                                          \"Frequent words of rating 6 to 10\"])\n",
    "fig.append_trace(trace0, 1, 1)\n",
    "fig.append_trace(trace1, 1, 2)\n",
    "fig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\n",
    "py.iplot(fig, filename='word-plots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "03e56f5eb12b5d2e738191a2ebfb61a85a2b963a"
   },
   "outputs": [],
   "source": [
    "freq_dict = defaultdict(int)\n",
    "for sent in df_all_1_5[\"review\"]:\n",
    "    for word in generate_ngrams(sent,2):\n",
    "        freq_dict[word] += 1\n",
    "fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n",
    "fd_sorted.columns = [\"word\", \"wordcount\"]\n",
    "trace1 = horizontal_bar_chart(fd_sorted.head(50), 'orange')\n",
    "\n",
    "freq_dict = defaultdict(int)\n",
    "for sent in df_all_6_10[\"review\"]:\n",
    "    for word in generate_ngrams(sent,2):\n",
    "        freq_dict[word] += 1\n",
    "fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n",
    "fd_sorted.columns = [\"word\", \"wordcount\"]\n",
    "trace2 = horizontal_bar_chart(fd_sorted.head(50), 'orange')\n",
    "\n",
    "# Creating two subplots\n",
    "fig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,horizontal_spacing=0.15,\n",
    "                          subplot_titles=[\"Frequent biagrams of rating 1 to 5\", \n",
    "                                          \"Frequent biagrams of rating 6 to 10\"])\n",
    "fig.append_trace(trace1, 1, 1)\n",
    "fig.append_trace(trace2, 1, 2)\n",
    "fig['layout'].update(height=1200, width=1000, paper_bgcolor='rgb(233,233,233)', title=\"Bigram Count Plots\")\n",
    "py.iplot(fig, filename='word-plots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "cae502a170fe1665fd9aff62a257c04d111887b8"
   },
   "outputs": [],
   "source": [
    "freq_dict = defaultdict(int)\n",
    "for sent in df_all_1_5[\"review\"]:\n",
    "    for word in generate_ngrams(sent,3):\n",
    "        freq_dict[word] += 1\n",
    "fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n",
    "fd_sorted.columns = [\"word\", \"wordcount\"]\n",
    "trace1 = horizontal_bar_chart(fd_sorted.head(50), 'green')\n",
    "\n",
    "freq_dict = defaultdict(int)\n",
    "for sent in df_all_6_10[\"review\"]:\n",
    "    for word in generate_ngrams(sent,3):\n",
    "        freq_dict[word] += 1\n",
    "fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n",
    "fd_sorted.columns = [\"word\", \"wordcount\"]\n",
    "trace2 = horizontal_bar_chart(fd_sorted.head(50), 'green')\n",
    "\n",
    "# Creating two subplots\n",
    "fig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,horizontal_spacing=0.15,\n",
    "                          subplot_titles=[\"Frequent trigrams of rating 1 to 5\", \n",
    "                                          \"Frequent trigrams of rating 6 to 10\"])\n",
    "fig.append_trace(trace1, 1, 1)\n",
    "fig.append_trace(trace2, 1, 2)\n",
    "fig['layout'].update(height=1200, width=1600, paper_bgcolor='rgb(233,233,233)', title=\"Trigram Count Plots\")\n",
    "py.iplot(fig, filename='word-plots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "b59a9e02961f89349684d69e793fc3270b103022"
   },
   "outputs": [],
   "source": [
    "freq_dict = defaultdict(int)\n",
    "for sent in df_all_1_5[\"review\"]:\n",
    "    for word in generate_ngrams(sent,4):\n",
    "        freq_dict[word] += 1\n",
    "fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n",
    "fd_sorted.columns = [\"word\", \"wordcount\"]\n",
    "trace1 = horizontal_bar_chart(fd_sorted.head(50), 'red')\n",
    "\n",
    "freq_dict = defaultdict(int)\n",
    "for sent in df_all_6_10[\"review\"]:\n",
    "    for word in generate_ngrams(sent,4):\n",
    "        freq_dict[word] += 1\n",
    "fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n",
    "fd_sorted.columns = [\"word\", \"wordcount\"]\n",
    "trace2 = horizontal_bar_chart(fd_sorted.head(50), 'red')\n",
    "\n",
    "# Creating two subplots\n",
    "fig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,horizontal_spacing=0.15,\n",
    "                          subplot_titles=[\"Frequent 4-grams of rating 1 to 5\", \n",
    "                                          \"Frequent 4-grams of rating 6 to 10\"])\n",
    "fig.append_trace(trace1, 1, 1)\n",
    "fig.append_trace(trace2, 1, 2)\n",
    "fig['layout'].update(height=1200, width=1600, paper_bgcolor='rgb(233,233,233)', title=\"4-grams Count Plots\")\n",
    "py.iplot(fig, filename='word-plots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6d119068cf249e999a8f216e0167af8e628fd70a"
   },
   "outputs": [],
   "source": [
    "rating = df_all['rating'].value_counts().sort_values(ascending=False)\n",
    "rating.plot(kind=\"bar\", figsize = (14,6), fontsize = 10,color=\"green\")\n",
    "plt.xlabel(\"\", fontsize = 20)\n",
    "plt.ylabel(\"\", fontsize = 20)\n",
    "plt.title(\"Count of rating values\", fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "14b215ad07c7d5733a4d950043d17f905732c222"
   },
   "outputs": [],
   "source": [
    "\n",
    "cnt_srs = df_all['date'].dt.year.value_counts()\n",
    "cnt_srs = cnt_srs.sort_index()\n",
    "plt.figure(figsize=(14,6))\n",
    "sns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color='green')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.xlabel('year', fontsize=12)\n",
    "plt.ylabel('', fontsize=12)\n",
    "plt.title(\"Number of reviews in year\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "16dcf7a747d2eb06895c49ee6fd476e179aed2ba"
   },
   "outputs": [],
   "source": [
    "df_all['year'] = df_all['date'].dt.year\n",
    "rating = df_all.groupby('year')['rating'].mean()\n",
    "rating.plot(kind=\"bar\", figsize = (14,6), fontsize = 10,color=\"green\")\n",
    "plt.xlabel(\"\", fontsize = 20)\n",
    "plt.ylabel(\"\", fontsize = 20)\n",
    "plt.title(\"Mean rating in year\", fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7dfdffd170217e7d753ea6d28a6630b7f494041c"
   },
   "outputs": [],
   "source": [
    "\n",
    "cnt_srs = df_all['date'].dt.month.value_counts()\n",
    "cnt_srs = cnt_srs.sort_index()\n",
    "plt.figure(figsize=(14,6))\n",
    "sns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color='green')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.xlabel('month', fontsize=12)\n",
    "plt.ylabel('', fontsize=12)\n",
    "plt.title(\"Number of reviews in month\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0a6c95402fb6d936ea4799d93f2c9eeab47585db"
   },
   "outputs": [],
   "source": [
    "df_all['month'] = df_all['date'].dt.month\n",
    "rating = df_all.groupby('month')['rating'].mean()\n",
    "rating.plot(kind=\"bar\", figsize = (14,6), fontsize = 10,color=\"green\")\n",
    "plt.xlabel(\"\", fontsize = 20)\n",
    "plt.ylabel(\"\", fontsize = 20)\n",
    "plt.title(\"Mean rating in month\", fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "808970cc32f9a7e573ab81b54b2afe12f86729c4"
   },
   "outputs": [],
   "source": [
    "df_all['day'] = df_all['date'].dt.day\n",
    "rating = df_all.groupby('day')['rating'].mean()\n",
    "rating.plot(kind=\"bar\", figsize = (14,6), fontsize = 10,color=\"green\")\n",
    "plt.xlabel(\"\", fontsize = 20)\n",
    "plt.ylabel(\"\", fontsize = 20)\n",
    "plt.title(\"Mean rating in day\", fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "46c8c44e5e61e6a4139db9cbc9b0c10533923f7e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0b8c382bf6e31efc48656e098dfefc76cf88deff"
   },
   "outputs": [],
   "source": [
    "df_all[\"usefulCount\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "541ea018457f9107abf3f2c11051b39b2b17198d"
   },
   "source": [
    "### 1.3 Missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fe78d3aec92fbac8f64723535bb47c09a40231c3"
   },
   "outputs": [],
   "source": [
    "percent = (df_all.isnull().sum()).sort_values(ascending=False)\n",
    "percent.plot(kind=\"bar\", figsize = (14,6), fontsize = 10, color='green')\n",
    "plt.xlabel(\"Columns\", fontsize = 20)\n",
    "plt.ylabel(\"\", fontsize = 20)\n",
    "plt.title(\"Total Missing Value \", fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bcbd81bf2a4013c6ce9cb81067ab15684171203c"
   },
   "outputs": [],
   "source": [
    "print(\"Missing value (%):\", 1200/df_all.shape[0] *100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0c9a363d9fc7b266ee9c8a390d3c841b71126037"
   },
   "source": [
    "## 2. Date Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "32e167512298155137e85532b01158049819c100"
   },
   "source": [
    "### 2.1. Missing Values Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "af5d3f3c7fccd92a93413514030b6a33b0a89052"
   },
   "outputs": [],
   "source": [
    "df_train = df_train.dropna(axis=0)\n",
    "df_test = df_test.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1012e7c255ad73cf21048fd6534cb20bbcf6f1ce"
   },
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_train,df_test]).reset_index()\n",
    "del df_all['index']\n",
    "percent = (df_all.isnull().sum()).sort_values(ascending=False)\n",
    "percent.plot(kind=\"bar\", figsize = (14,6), fontsize = 10, color='green')\n",
    "plt.xlabel(\"Columns\", fontsize = 20)\n",
    "plt.ylabel(\"\", fontsize = 20)\n",
    "plt.title(\"Total Missing Value \", fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "aaceaccc91bf0a846df8a6377558d89824a01c9c"
   },
   "source": [
    "### 2.2 Condition Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "127f1391018fc3049ea14df7e5ea43c96812a9c3"
   },
   "source": [
    "We will delete the sentences with the form above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f4269e93fb9a64b79f9e74db338d4df28e2ba0f8"
   },
   "outputs": [],
   "source": [
    "all_list = set(df_all.index)\n",
    "span_list = []\n",
    "for i,j in enumerate(df_all['condition']):\n",
    "    if '</span>' in j:\n",
    "        span_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8545b5cdc904897a886bfad95fadf0196515f420"
   },
   "outputs": [],
   "source": [
    "new_idx = all_list.difference(set(span_list))\n",
    "df_all = df_all.iloc[list(new_idx)].reset_index()\n",
    "del df_all['index']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b1a5f5508ba8367b0e1fe86bbd9356965d104a98"
   },
   "source": [
    "Next, we will delete conditions with only one drug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "13324f1aaed4de4d20c53cd52a6ee2dca301d42a"
   },
   "outputs": [],
   "source": [
    "df_condition = df_all.groupby(['condition'])['drugName'].nunique().sort_values(ascending=False)\n",
    "df_condition = pd.DataFrame(df_condition).reset_index()\n",
    "df_condition.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3a1341f12498cbbd65e8e7cd1ab9789e94f5468e"
   },
   "outputs": [],
   "source": [
    "df_condition_1 = df_condition[df_condition['drugName']==1].reset_index()\n",
    "df_condition_1['condition'][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "edf19fcd7a99264a5436034d648b20d0bff2fa7b"
   },
   "outputs": [],
   "source": [
    "all_list = set(df_all.index)\n",
    "condition_list = []\n",
    "for i,j in enumerate(df_all['condition']):\n",
    "    for c in list(df_condition_1['condition']):\n",
    "        if j == c:\n",
    "            condition_list.append(i)\n",
    "            \n",
    "new_idx = all_list.difference(set(condition_list))\n",
    "df_all = df_all.iloc[list(new_idx)].reset_index()\n",
    "del df_all['index']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cb80a22040f4f9ca03311ce9e1ea782eb8ee89ad"
   },
   "source": [
    "### 2.3 Review Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5afbc9c8b87abc646ed24fb5a0feadd5996169a1"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "da532e45210e8a15eb1361aad88fc749eebea620"
   },
   "source": [
    "- \\r\\n : we need to convert html grammer\n",
    "- ... , &#039; : deal with not alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "6f546ffb47ae31a30903d4075e8c514a45c7a951"
   },
   "outputs": [],
   "source": [
    "stops = set(stopwords.words('english'))\n",
    "#stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "dde27caf5b374b0f3f0c2acd9d038daba43c4c31"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "def plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), \n",
    "                   title = None, title_size=40, image_color=False):\n",
    "    stopwords = set(STOPWORDS)\n",
    "    more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}\n",
    "    stopwords = stopwords.union(more_stopwords)\n",
    "\n",
    "    wordcloud = WordCloud(background_color='white',\n",
    "                    stopwords = stopwords,\n",
    "                    max_words = max_words,\n",
    "                    max_font_size = max_font_size, \n",
    "                    random_state = 42,\n",
    "                    width=800, \n",
    "                    height=400,\n",
    "                    mask = mask)\n",
    "    wordcloud.generate(str(text))\n",
    "    \n",
    "    plt.figure(figsize=figure_size)\n",
    "    if image_color:\n",
    "        image_colors = ImageColorGenerator(mask);\n",
    "        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n",
    "        plt.title(title, fontdict={'size': title_size,  \n",
    "                                  'verticalalignment': 'bottom'})\n",
    "    else:\n",
    "        plt.imshow(wordcloud);\n",
    "        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n",
    "                                  'verticalalignment': 'bottom'})\n",
    "    plt.axis('off');\n",
    "    plt.tight_layout()  \n",
    "    \n",
    "plot_wordcloud(stops, title=\"Word Cloud of stops\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "275f9e83f9935d0891fa11a6440b6caf4604e5d3"
   },
   "outputs": [],
   "source": [
    "not_stop = [\"aren't\",\"couldn't\",\"didn't\",\"doesn't\",\"don't\",\"hadn't\",\"hasn't\",\"haven't\",\"isn't\",\"mightn't\",\"mustn't\",\"needn't\",\"no\",\"nor\",\"not\",\"shan't\",\"shouldn't\",\"wasn't\",\"weren't\",\"wouldn't\"]\n",
    "for i in not_stop:\n",
    "    stops.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "386e3e954a0b1db1d5e7e739f6928ba245e3b9c0"
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, metrics, ensemble, naive_bayes, linear_model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import lightgbm as lgb\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_columns = 999\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8368a97c8e435adb9f07d144f494fc60a083a5a1"
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def review_to_words(raw_review):\n",
    "    # 1. Delete HTML \n",
    "    review_text = BeautifulSoup(raw_review, 'html.parser').get_text()\n",
    "    # 2. Make a space\n",
    "    letters_only = re.sub('[^a-zA-Z]', ' ', review_text)\n",
    "    # 3. lower letters\n",
    "    words = letters_only.lower().split()\n",
    "    # 5. Stopwords \n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    # 6. Stemming\n",
    "    stemming_words = [stemmer.stem(w) for w in meaningful_words]\n",
    "    # 7. space join words\n",
    "    return( ' '.join(stemming_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "be46dc37dd26328efedf9ca166b5635e0f13fb53"
   },
   "outputs": [],
   "source": [
    "%time df_all['review_clean'] = df_all['review'].apply(review_to_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6f6301c9d03a198e3ce7b22b172970152ee21b6a"
   },
   "source": [
    "### 3.1. Deep Learning Model Using N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6adae84cb2d9d4e0588cb10033c17447bf976d58"
   },
   "outputs": [],
   "source": [
    "# Make a rating\n",
    "df_all['sentiment'] = df_all[\"rating\"].apply(lambda x: 1 if x > 5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ba7f1a884407f9247915b3b6757219501cf13a15"
   },
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df_all, test_size=0.33, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d6a9555e8c90e09fab2edaf2107c6b6bfcb588fd"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = 'word', \n",
    "                             tokenizer = None,\n",
    "                             preprocessor = None, \n",
    "                             stop_words = None, \n",
    "                             min_df = 2,\n",
    "                             ngram_range=(4, 4),\n",
    "                             max_features = 20000\n",
    "                            )\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1b983fc4de6f9e7e3be07445c418ef1060d6f02f"
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', vectorizer),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3b1e9c628a7dec80e1e863f7acffdae64e3ae7ab"
   },
   "outputs": [],
   "source": [
    "%time train_data_features = pipeline.fit_transform(df_train['review_clean'])\n",
    "%time test_data_features = pipeline.fit_transform(df_test['review_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f71f9a7ad9cec9777814bf2746bc4237116b8b35"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Bidirectional, LSTM, BatchNormalization, Dropout\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ce3dfe83a105f9628b61ee107c3fed27f252fb40"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import random\n",
    "\n",
    "# 1. Dataset\n",
    "y_train = df_train['sentiment']\n",
    "y_test = df_test['sentiment']\n",
    "solution = y_test.copy()\n",
    "\n",
    "# 2. Model Structure\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Dense(200, input_shape=(20000,)))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "\n",
    "model.add(keras.layers.Dense(300))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "\n",
    "model.add(keras.layers.Dense(100, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# 3. Model compile\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ea8db26bcd3cba382ff381684b3f229d381bca5f"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "48b6724a5d7b994c15bd4505bccdb6b3e602ba21"
   },
   "outputs": [],
   "source": [
    "# 4. Train model\n",
    "hist = model.fit(train_data_features, y_train, epochs=10, batch_size=64)\n",
    "\n",
    "# 5. Traing process\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.set_ylim([0.0, 1.0])\n",
    "acc_ax.set_ylim([0.0, 1.0])\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 6. Evaluation\n",
    "loss_and_metrics = model.evaluate(test_data_features, y_test, batch_size=32)\n",
    "print('loss_and_metrics : ' + str(loss_and_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d678d5dd391fc455de2cad0b4b8f6fe76374583c"
   },
   "outputs": [],
   "source": [
    "sub_preds_deep = model.predict(test_data_features,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score, roc_curve, auc\n",
    "false_positive_rate, recall, thresholds = roc_curve(y_test,sub_preds_deep )\n",
    "roc_auc = auc(false_positive_rate, recall)\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.plot(false_positive_rate, recall, 'b', label = 'AUC = %0.3f' %roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1], [0,1], 'r--')\n",
    "plt.xlim([0.0,1.0])\n",
    "plt.ylim([0.0,1.0])\n",
    "plt.ylabel('Recall')\n",
    "plt.xlabel('Fall-out (1-Specificity)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "603ee53139269ed2a9b009a65a9b026b9ce11bf4"
   },
   "source": [
    "### 3.2 Lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fee63f2564ce6ad240d6b2e3d1aaf028e98c15a8"
   },
   "source": [
    "To improve the low accuracy, we will use machine learning. First of all, this is the sentiment analysis model using only usefulCount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "2c4d7f24d1f25d99979687c2101f60bf202d2f04"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve, average_precision_score\n",
    "from sklearn.model_selection import KFold\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#folds = KFold(n_splits=5, shuffle=True, random_state=546789)\n",
    "target = df_train['sentiment']\n",
    "feats = ['usefulCount']\n",
    "\n",
    "sub_preds = np.zeros(df_test.shape[0])\n",
    "\n",
    "trn_x, val_x, trn_y, val_y = train_test_split(df_train[feats], target, test_size=0.2, random_state=42) \n",
    "feature_importance_df = pd.DataFrame() \n",
    "    \n",
    "clf = LGBMClassifier(\n",
    "        n_estimators=2000,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=30,\n",
    "        #colsample_bytree=.9,\n",
    "        subsample=.9,\n",
    "        max_depth=7,\n",
    "        reg_alpha=.1,\n",
    "        reg_lambda=.1,\n",
    "        min_split_gain=.01,\n",
    "        min_child_weight=2,\n",
    "        silent=-1,\n",
    "        verbose=-1,\n",
    "        )\n",
    "        \n",
    "clf.fit(trn_x, trn_y, \n",
    "        eval_set= [(trn_x, trn_y), (val_x, val_y)], \n",
    "        verbose=100, early_stopping_rounds=100  #30\n",
    "    )\n",
    "\n",
    "sub_preds = clf.predict(df_test[feats])\n",
    "        \n",
    "fold_importance_df = pd.DataFrame()\n",
    "fold_importance_df[\"feature\"] = feats\n",
    "fold_importance_df[\"importance\"] = clf.feature_importances_\n",
    "feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1684caab656ca830228cc230871ac3e3d961352a"
   },
   "outputs": [],
   "source": [
    "solution = df_test['sentiment']\n",
    "confusion_matrix(y_pred=sub_preds, y_true=solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "29474d09e5ec8832fbacc39d5738472c8470d768"
   },
   "source": [
    "We will add variables for higher accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8e6032557347dab3092ecfa45baec6f214b49017"
   },
   "outputs": [],
   "source": [
    "len_train = df_train.shape[0]\n",
    "df_all = pd.concat([df_train,df_test])\n",
    "del df_train, df_test;\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0ae19226fc6ec7c02e0510a51ec7dd05724da876"
   },
   "outputs": [],
   "source": [
    "df_all['date'] = pd.to_datetime(df_all['date'])\n",
    "df_all['day'] = df_all['date'].dt.day\n",
    "df_all['year'] = df_all['date'].dt.year\n",
    "df_all['month'] = df_all['date'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bd4f0e2c792a6d084979c548596df92848d06856"
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from tqdm import tqdm\n",
    "reviews = df_all['review_clean']\n",
    "\n",
    "Predict_Sentiment = []\n",
    "for review in tqdm(reviews):\n",
    "    blob = TextBlob(review)\n",
    "    Predict_Sentiment += [blob.sentiment.polarity]\n",
    "df_all[\"Predict_Sentiment\"] = Predict_Sentiment\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1091f4a36ae81c2973ecdb6fac964c646d93ba0f"
   },
   "outputs": [],
   "source": [
    "np.corrcoef(df_all[\"Predict_Sentiment\"], df_all[\"rating\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6deb741f0941fa04ac1a87e9343f9079b5c1a83c"
   },
   "outputs": [],
   "source": [
    "np.corrcoef(df_all[\"Predict_Sentiment\"], df_all[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "38f83afcdd82bb469b8d6b6b90e9b60b6921219e"
   },
   "outputs": [],
   "source": [
    "reviews = df_all['review']\n",
    "\n",
    "Predict_Sentiment = []\n",
    "for review in tqdm(reviews):\n",
    "    blob = TextBlob(review)\n",
    "    Predict_Sentiment += [blob.sentiment.polarity]\n",
    "df_all[\"Predict_Sentiment2\"] = Predict_Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8952070fd762f0d27667940b8a9c764a75eff67e"
   },
   "outputs": [],
   "source": [
    "np.corrcoef(df_all[\"Predict_Sentiment2\"], df_all[\"rating\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "69fa8972589337694b89e823a10e8134cd0b8ed2"
   },
   "outputs": [],
   "source": [
    "np.corrcoef(df_all[\"Predict_Sentiment2\"], df_all[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6488abff05ebf6698e01cfae7550d977fb8a6b02"
   },
   "outputs": [],
   "source": [
    "\n",
    "df_all['count_sent']=df_all[\"review\"].apply(lambda x: len(re.findall(\"\\n\",str(x)))+1)\n",
    "\n",
    "#Word count in each comment:\n",
    "df_all['count_word']=df_all[\"review_clean\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "#Unique word count\n",
    "df_all['count_unique_word']=df_all[\"review_clean\"].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "#Letter count\n",
    "df_all['count_letters']=df_all[\"review_clean\"].apply(lambda x: len(str(x)))\n",
    "\n",
    "#punctuation count\n",
    "df_all[\"count_punctuations\"] = df_all[\"review\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "\n",
    "#upper case words count\n",
    "df_all[\"count_words_upper\"] = df_all[\"review\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "\n",
    "#title case words count\n",
    "df_all[\"count_words_title\"] = df_all[\"review\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "\n",
    "#Number of stopwords\n",
    "df_all[\"count_stopwords\"] = df_all[\"review\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stops]))\n",
    "\n",
    "#Average length of the words\n",
    "df_all[\"mean_word_len\"] = df_all[\"review_clean\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cfeeba96c727574a58ade17ffdab2f1d7d7ebf62"
   },
   "source": [
    "We added a season variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6a4129d1c7c60b57014db60a990bbeef5e73e223"
   },
   "outputs": [],
   "source": [
    "df_all['season'] = df_all[\"month\"].apply(lambda x: 1 if ((x>2) & (x<6)) else(2 if (x>5) & (x<9) else (3 if (x>8) & (x<12) else 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "025efc594ab90e1c2e4c0f32a3eca56aa063976c"
   },
   "source": [
    "We normalized useful count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "d001d32f4216747d3ad768c3d136ab4974c21f0f"
   },
   "outputs": [],
   "source": [
    "df_train = df_all[:len_train]\n",
    "df_test = df_all[len_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "99577a2f98a7e997ff0d1c317da32219cbadf506"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve, average_precision_score\n",
    "from sklearn.model_selection import KFold\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "#folds = KFold(n_splits=5, shuffle=True, random_state=546789)\n",
    "target = df_train['sentiment']\n",
    "feats = ['usefulCount','day','year','month','Predict_Sentiment','Predict_Sentiment2', 'count_sent',\n",
    " 'count_word', 'count_unique_word', 'count_letters', 'count_punctuations',\n",
    " 'count_words_upper', 'count_words_title', 'count_stopwords', 'mean_word_len', 'season']\n",
    "\n",
    "sub_preds = np.zeros(df_test.shape[0])\n",
    "\n",
    "trn_x, val_x, trn_y, val_y = train_test_split(df_train[feats], target, test_size=0.2, random_state=42) \n",
    "feature_importance_df = pd.DataFrame() \n",
    "    \n",
    "clf = LGBMClassifier(\n",
    "        n_estimators=10000,\n",
    "        learning_rate=0.10,\n",
    "        num_leaves=30,\n",
    "        #colsample_bytree=.9,\n",
    "        subsample=.9,\n",
    "        max_depth=7,\n",
    "        reg_alpha=.1,\n",
    "        reg_lambda=.1,\n",
    "        min_split_gain=.01,\n",
    "        min_child_weight=2,\n",
    "        silent=-1,\n",
    "        verbose=-1,\n",
    "        )\n",
    "        \n",
    "clf.fit(trn_x, trn_y, \n",
    "        eval_set= [(trn_x, trn_y), (val_x, val_y)], \n",
    "        verbose=100, early_stopping_rounds=100  #30\n",
    "    )\n",
    "\n",
    "sub_preds = clf.predict(df_test[feats])\n",
    "        \n",
    "fold_importance_df = pd.DataFrame()\n",
    "fold_importance_df[\"feature\"] = feats\n",
    "fold_importance_df[\"importance\"] = clf.feature_importances_\n",
    "feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a666baea3f58fe9c6d5292cfe24391f0ebb903e8"
   },
   "outputs": [],
   "source": [
    "confusion_matrix(y_pred=sub_preds, y_true=solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positive_rate, recall, thresholds = roc_curve(solution, sub_preds)\n",
    "roc_auc = auc(false_positive_rate, recall)\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.plot(false_positive_rate, recall, 'b',color='orange', label = 'AUC = %0.3f' %roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1], [0,1], 'r--')\n",
    "plt.xlim([0.0,1.0])\n",
    "plt.ylim([0.0,1.0])\n",
    "plt.ylabel('Recall')\n",
    "plt.xlabel('Fall-out (1-Specificity)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "208f503ba23bc2d8412b27c61772371315ffef78"
   },
   "outputs": [],
   "source": [
    "cols = feature_importance_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
    "    by=\"importance\", ascending=False)[:50].index\n",
    "\n",
    "best_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n",
    "\n",
    "plt.figure(figsize=(14,10))\n",
    "sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "plt.title('LightGBM Features (avg over folds)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('lgbm_importances.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c8feb72b512e8032016da05083eebe80c184c58a"
   },
   "source": [
    "### 3.3 Dictionary_Sentiment_Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "62f0f62033a5f3661c119213ed322b880a545da4"
   },
   "outputs": [],
   "source": [
    "# import dictionary data\n",
    "word_table = pd.read_csv(\"../input/dictionary/inquirerbasic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e3ea9c71d4d8c47a991b2ddd11143699af5c6ac0"
   },
   "outputs": [],
   "source": [
    "word_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "384ba6b02f776f9080d57c18a7f3740b3a2236d3"
   },
   "outputs": [],
   "source": [
    "##1. make list of sentiment\n",
    "#Positiv word list   \n",
    "temp_Positiv = []\n",
    "Positiv_word_list = []\n",
    "for i in range(0,len(word_table.Positiv)):\n",
    "    if word_table.iloc[i,2] == \"Positiv\":\n",
    "        temp = word_table.iloc[i,0].lower()\n",
    "        temp1 = re.sub('\\d+', '', temp)\n",
    "        temp2 = re.sub('#', '', temp1) \n",
    "        temp_Positiv.append(temp2)\n",
    "\n",
    "Positiv_word_list = list(set(temp_Positiv))\n",
    "len(temp_Positiv)\n",
    "len(Positiv_word_list)  #del temp_Positiv\n",
    "\n",
    "#Negativ word list          \n",
    "temp_Negativ = []\n",
    "Negativ_word_list = []\n",
    "for i in range(0,len(word_table.Negativ)):\n",
    "    if word_table.iloc[i,3] == \"Negativ\":\n",
    "        temp = word_table.iloc[i,0].lower()\n",
    "        temp1 = re.sub('\\d+', '', temp)\n",
    "        temp2 = re.sub('#', '', temp1) \n",
    "        temp_Negativ.append(temp2)\n",
    "\n",
    "Negativ_word_list = list(set(temp_Negativ))\n",
    "len(temp_Negativ)\n",
    "len(Negativ_word_list)  #del temp_Negativ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "73ac447f7fad4fdef122155d74cbe58e00fc57ec"
   },
   "source": [
    "We counted the number of words in review_clean which are included in dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bddbe0aacbe8480869d1935d49bac6397827cd6c"
   },
   "outputs": [],
   "source": [
    "##2. counting the word 98590\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(vocabulary = Positiv_word_list)\n",
    "content = df_test['review_clean']\n",
    "X = vectorizer.fit_transform(content)\n",
    "f = X.toarray()\n",
    "f = pd.DataFrame(f)\n",
    "f.columns=Positiv_word_list\n",
    "df_test[\"num_Positiv_word\"] = f.sum(axis=1)\n",
    "\n",
    "vectorizer2 = CountVectorizer(vocabulary = Negativ_word_list)\n",
    "content = df_test['review_clean']\n",
    "X2 = vectorizer2.fit_transform(content)\n",
    "f2 = X2.toarray()\n",
    "f2 = pd.DataFrame(f2)\n",
    "f2.columns=Negativ_word_list\n",
    "df_test[\"num_Negativ_word\"] = f2.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "63f678116cb243d737982bb6ae8e9dbd3da49094",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##3. decide sentiment\n",
    "df_test[\"Positiv_ratio\"] = df_test[\"num_Positiv_word\"]/(df_test[\"num_Positiv_word\"]+df_test[\"num_Negativ_word\"])\n",
    "df_test[\"sentiment_by_dic\"] = df_test[\"Positiv_ratio\"].apply(lambda x: 1 if (x>=0.5) else (0 if (x<0.5) else 0.5))\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "da172c9523937834f34ba3d2a137d73c0ad41443"
   },
   "source": [
    "We defined Positiv_ratio = the number of positive words / (the number of positive words+the number of negative words) If the ratio is lower than 0.5, we classified as negative and if it's higher than 0.5, we classified as positive. With remainders, we classified as neutral, which includes the sentence without either positive or negative words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ebc76b1ffbc7eafc6632f8c89949040812a704eb"
   },
   "source": [
    "As mentioned earlier, we have normalized usefulCount by condition to solve the problem that usefulCount shows bias depending on condition. You can then add three predicted emotion values and multiply them by the normalized usefulCount to get the predicted value.\n",
    "\n",
    "Now, we can recommend drug by condition in order of final predicted value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "85cf8b344e09601d6fb5951ece7c9e2601c3ed8d",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def userful_count(data):\n",
    "    grouped = data.groupby(['condition']).size().reset_index(name='user_size')\n",
    "    data = pd.merge(data,grouped,on='condition',how='left')\n",
    "    return data\n",
    "#___________________________________________________________\n",
    "df_test =  userful_count(df_test) \n",
    "df_test['usefulCount'] = df_test['usefulCount']/df_test['user_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ff0e4b2e3a4c9d6b82b2f32a9ac8601f2a18573d"
   },
   "outputs": [],
   "source": [
    "df_test['deep_pred'] = sub_preds_deep\n",
    "df_test['machine_pred'] = sub_preds\n",
    "\n",
    "df_test['total_pred'] = (df_test['deep_pred'] + df_test['machine_pred'] + df_test['sentiment_by_dic'])*df_test['usefulCount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "46e8ff4f7569243d85c489e5034e37850ce623f3"
   },
   "outputs": [],
   "source": [
    "df_test = df_test.groupby(['condition','drugName']).agg({'total_pred' : ['mean']})\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
